EXPERIMENT 1
-------------

=> AIM
Write a Python Program to Plot Frequency   Polygon, Ogive, and Histogram of the Unemployment Rates Data provided to you.


=> THEORY
1) Frequency Polygon: A Frequency Polygon is a graphical representation of a data set, which displays the distribution of values. It is a line graph drawn by connecting the midpoints of the tops of the bars in a histogram.

2)Histogram: A Histogram is a graphical representation of the distribution of a dataset. It consists of a set of contiguous bars where the height of each bar corresponds to the frequency (or relative frequency) of the data within a specific range.

3)Ogive: An Ogive is a graph that represents the cumulative frequency of a dataset. It is constructed by plotting points representing the cumulative frequency at the upper class boundaries of the data.


=> CODE
import matplotlib.pyplot as plt 
import numpy as np 
# Unemployment Rates Data 
data = np.array([ 
    1.6, 2.1, 4.2, 8.6, 9.6, 
    1.5, 2.7, 4.6, 10, 10.4, 
    1.2, 2.3, 5.2, 10.5, 11.8, 
    1.4, 2.5, 5.4, 10.6, 12.3, 
    1.6, 2.8, 6.1, 10.8, 11.8, 
    1.2, 2.9, 6.5, 10.3, 12.5, 
    1.6, 2.8, 7.6, 9.6, 12.4, 
    1.6, 2.9, 8.3, 9.1, 11.8 
]) 
# Frequency Polygon 
plt.figure(figsize=(10, 5)) 
plt.hist(data, bins=8, density=True, alpha=0.7, color='blue', edgecolor='black') 
plt.title('Frequency Polygon') 
plt.xlabel('Unemployment Rates') 
plt.ylabel('Frequency') 
plt.grid(True) 
plt.show() 
# Ogive 
cumulative_freq = np.cumsum(np.histogram(data, bins=8)[0]) 
plt.figure(figsize=(10, 5)) 
plt.plot(np.histogram(data, bins=8)[1][1:], cumulative_freq, marker='o', color='green') 
plt.title('Ogive') 
plt.xlabel('Unemployment Rates') 
plt.ylabel('Cumulative Frequency') 
plt.grid(True) 
plt.show() 
# Histogram 
plt.figure(figsize=(10, 5)) 
plt.hist(data, bins=8, color='purple', edgecolor='black') 
plt.title('Histogram') 
plt.xlabel('Unemployment Rates') 
plt.ylabel('Frequency') 
plt.grid(True) 
plt.show()





EXPERIMENT 2
-------------

=> AIM
Write Python Program to generate some random data. Plot Box plots using this random data.


=> THEORY
1) Generating Random Data: In statistical analysis and data science, it is common to generate random data for various purposes, such as testing algorithms, simulating scenarios, or creating sample datasets for analysis. 

2) Boxplots: A Boxplot (or Box-and-Whisker plot) is a standardized way of displaying the distribution of data 
based on a five-number summary:  
  A. Minimum: The smallest data point within 1.5 times the interquartile range (IQR) below the first quartile.  
  B. First Quartile (Q1): The 25th percentile of the data.  
  C. Median (Q2): The middle value of the dataset.  
  D. Third Quartile (Q3): The 75th percentile of the data.  
  E. Maximum: The largest data point within 1.5 times the IQR above the third quartile.


=> CODE
import numpy as np 
import matplotlib.pyplot as plt 
# Set a seed for reproducibility 
np.random.seed(42) 
# Generate random data for three groups 
data_group1 = np.random.normal(0, 1, 100) 
data_group2 = np.random.normal(2, 1, 100) 
data_group3 = np.random.normal(-2, 1, 100) 
# Combine the data into a list 
data = [data_group1, data_group2, data_group3] 
# Create a box plot 
plt.boxplot(data, labels=['Group 1', 'Group 2', 'Group 3']) 
plt.title('Box Plot of Random Data') 
plt.xlabel('Groups') 
plt.ylabel('Values') 
plt.grid(True) 
plt.show()





EXPEERIMENT 3
--------------

=> AIM
Write Python Program to generate  data from Binomial  distribution  and show the pdf plot for various of n and p.


=> THEORY
Binomial Distribution: The Binomial Distribution is a discrete probability distribution that models the number of successes in a fixed number of independent and identical Bernoulli trials, where each trial results in a success or failure with a constant probability of success, denoted by \(p\). The distribution is characterized by two parameters:
   \(n\), the number of trials, and
   \(p\), the probability of success on a single trial. 
 
The probability mass function (PMF) of the Binomial distribution is given by: 
  P(X=k) = ( k/n  )⋅ p^k ⋅ (1−p)^n−k ,
where X is the random variable representing the number of successes, \(k\) is a specific number of successes, and \(\binom{n}{k}\) is the binomial coefficient, 
representing the number of ways to choose \(k\) successes from \(n\) trials. 
 
The Binomial Distribution is commonly applied in scenarios involving a fixed number of trials with only two possible outcomes, such as success or failure, yes or no. It serves as a fundamental model in probability theory and statistics, and it finds applications in various fields, including quality control, genetics, and finance.


=> CODE
import numpy as np 
import matplotlib.pyplot as plt 
from scipy.stats import binom 
 
# Set a seed for reproducibility 
np.random.seed(42) 
 
# Values for n and p 
n_values = [10, 20, 30] 
p_values = [0.2, 0.5, 0.8] 
 
# Create subplots 
fig, axs = plt.subplots(len(n_values), len(p_values), figsize=(12, 8), sharex=True, sharey=True) 
 
# Generate and plot data for each combination of n and p 
for i, n in enumerate(n_values): 
    for j, p in enumerate(p_values): 
        # Generate data from binomial distribution 
        data = np.random.binomial(n, p, 1000) 
         
        # Plot the PDF 
        x = np.arange(0, n + 1) 
        y = binom.pmf(x, n, p) 
        axs[i, j].vlines(x, 0, y, colors='b', lw=5) 
        axs[i, j].set_title(f'n={n}, p={p}') 
        axs[i, j].grid(True) 
 
# Set common labels 
fig.suptitle('Binomial Distribution - Probability Density Function', fontsize=16) 
plt.xlabel('Number of Successes') 
plt.ylabel('Probability Density') 
# Adjust layout 
plt.tight_layout(rect=[0, 0, 1, 0.96]) 
plt.show()





EXPERIMENT 4
-------------

=> AIM
Write a Python Program to generate Poisson distribution and show the pdf plot for various values of  Lambda.


=> THEORY
The Poisson distribution models the number of events occurring within a fixed interval of time or space when events happen independently and at a constant average rate. It is characterized by a single parameter, lambda (λ), representing the average rate of occurrences.  
The probability mass function is given by 
 P(X=k) = (e^(-λ) * λ^k) / k 
 describing the likelihood of k events. The distribution is widely used in various fields, including telecommunications, biology, and queueing theory, to model rare and unpredictable events.


=> CODE
import numpy as np 
import matplotlib.pyplot as plt 
from scipy.stats import poisson 
lambda_values = [2, 5, 8] 
 
fig, axs = plt.subplots(len(lambda_values), 1, figsize=(8, 6), sharex=True) 
for i, lmbda in enumerate(lambda_values): 
    # Generate data from Poisson distribution 
    data = np.random.poisson(lmbda, 1000) 
    x = np.arange(0, 15) 
    y = poisson.pmf(x, lmbda) 
    axs[i].vlines(x, 0, y, colors='b', lw=5) 
    axs[i].set_title(f'Lambda = {lmbda}') 
    axs[i].grid(True) 
fig.suptitle('Poisson Distribution - Probability Mass Function', fontsize=16) 
plt.xlabel('Number of Events') 
plt.ylabel('Probability Mass') 
plt.tight_layout(rect=[0, 0, 1, 0.96]) 
plt.show()





EXPERIMENT 5
-------------

=> AIM
Write Python Program to generate  data from Normal   distribution  and show the pdf plot for various of µ, σ.


=> THEORY
The Normal distribution, also known as the Gaussian distribution or bell curve, is a continuous probability distribution characterized by a symmetric, bell-shaped curve. It is fully described by two parameters: the mean (μ) and the standard deviation (σ).
In a standard Normal distribution (with mean 0 and standard deviation 1), about 68% of data falls within one standard deviation, 95% within two standard deviations, and 99.7% within three standard deviations. The Normal distribution is ubiquitous in statistics and represents a wide range of natural phenomena, facilitating various statistical analyses and hypothesis testing.


=> CODE
import numpy as np 
import matplotlib.pyplot as plt 
from scipy.stats import norm 
mean_values = [0, 2, 5] 
variance_values = [1, 3, 0.5] 
fig, axs = plt.subplots(len(mean_values), len(variance_values), figsize=(12, 8), sharex=True, sharey=True) 
for i, mean in enumerate(mean_values): 
    for j, variance in enumerate(variance_values): 
        # Generate data from normal distribution 
        data = np.random.normal(mean, np.sqrt(variance), 1000) 
        x = np.linspace(mean - 4*np.sqrt(variance), mean + 4*np.sqrt(variance), 100) 
        y = norm.pdf(x, mean, np.sqrt(variance)) 
        axs[i, j].plot(x, y, 'b-', lw=2) 
        axs[i, j].set_title(f'Mean={mean}, Variance={variance}') 
        axs[i, j].grid(True) 
fig.suptitle('Normal Distribution - Probability Density Function', fontsize=16) 
plt.xlabel('Values') 
plt.ylabel('Probability Density') 
plt.tight_layout(rect=[0, 0, 1, 0.96]) 
plt.show()





EXPERIMENT 6
------------

=> AIM
Write Python Program to generate exponential distribution and show the pdf plot for various values of  Lambda.


=> THEORY
The Exponential distribution describes the time between consecutive, independent events in a Poisson process, where events occur continuously and independently at a constant average rate. A single parameter characterizes it, the rate parameter (λ), which is the reciprocal of the average time between events. The probability density function is given by f(x) = λ * e^(-λx) for x >= 0, representing the likelihood of an event occurring at time x. The Exponential distribution is widely used in reliability engineering, queueing theory, and survival analysis to model the time until an event of interest occurs.


=> CODE
import numpy as np 
import matplotlib.pyplot as plt 
from scipy.stats import expon 
 
# Values for lambda 
lambda_values = [0.5, 1, 2] 
 
# Create subplots 
fig, axs = plt.subplots(len(lambda_values), 1, figsize=(8, 6), sharex=True) 
 
# Generate and plot data for each value of lambda 
for i, lmbda in enumerate(lambda_values): 
    # Generate data from exponential distribution 
    data = np.random.exponential(scale=1/lmbda, size=1000) 
     
    # Plot the PDF 
    x = np.linspace(0, 5/lmbda, 100) 
    y = expon.pdf(x, scale=1/lmbda) 
    axs[i].plot(x, y, 'b-', lw=2) 
    axs[i].set_title(f'Lambda = {lmbda}') 
    axs[i].grid(True) 
 
# Set common labels 
fig.suptitle('Exponential Distribution - Probability Density Function', fontsize=16) 
plt.xlabel('Values') 
plt.ylabel('Probability Density') 
 
# Adjust layout 
plt.tight_layout(rect=[0, 0, 1, 0.96]) 
plt.show()





EXPERIMENT 7
------------

=> AIM
Write a Python Program  to import and export data using Pandas. Demonstrate various data pre-processing techniques.


=> THEORY
Data pre-processing techniques: 
1. Handling Missing Data: - Impute or delete missing values to ensure completeness. - Choose appropriate methods based on the nature and extent of missing data. 
2. Handling Outliers: - Identify and address outliers using statistical methods or visualizations. - Transform or clip extreme values to prevent them from influencing results. 
3. Data Scaling: - Normalize or standardize features for consistent impact. - Methods like Min-Max scaling or Z-score normalization ensure comparabl scales. 
4. Encoding Categorical Variables: - Convert categorical variables to numerical representations. - Options include one-hot encoding or label encoding for model compatibility. 
5. Feature Engineering: - Create new or transform existing features for improved model performance. - Techniques include polynomial features, log transformations, or binning. 
6. Data Splitting: - Split data into training, validation, and testing sets. - Ensure representative distribution across sets for unbiased model evaluation.


=> CODE
import pandas as pd 
from sklearn import datasets 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler, LabelEncoder 
# Load the Iris dataset 
iris = datasets.load_iris() 
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names) 
iris_df['target'] = iris.target 
# Display the first few rows of the dataset 
print("Original Iris Dataset:") 
print(iris_df.head()) 
 
# Import the dataset from the CSV file 
imported_iris_df = pd.read_csv('iris_dataset.csv') 
 
# Display the first few rows of the imported dataset 
print("\nImported Iris Dataset:") 
print(imported_iris_df.head()) 
 
# Data Pre-processing Techniques 
# 1. Train-test Split 
X = iris_df.drop('target', axis=1) 
y = iris_df['target'] 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 
 
# 2. Standardization (Scaling) 
scaler = StandardScaler() 
X_train_scaled = scaler.fit_transform(X_train) 
X_test_scaled = scaler.transform(X_test) 
 
# Display the scaled features 
print("\nScaled Features (Standardization):") 
print(pd.DataFrame(data=X_train_scaled, columns=iris.feature_names).head()) 
 
# 3. Label Encoding 
le = LabelEncoder() 
y_train_encoded = le.fit_transform(y_train) 
y_test_encoded = le.transform(y_test) 
 
# Display the encoded target labels 
print("\nEncoded Target Labels:") 
print(pd.DataFrame(data={'original': y_train, 'encoded': y_train_encoded}).head()) 
 
# 4. Missing Data Handling (Not applicable for the Iris dataset as it doesn't contain missing values) 
 
# 5. Handling Categorical Data (Not applicable for the Iris dataset as it doesn't contain categorical features) 
 
# 6. Feature Engineering (Not applicable for the Iris dataset in this example) 
 
# Additional: Data Exploration 
# Summary Statistics 
print("\nSummary Statistics:") 
print(iris_df.describe()) 
 
# Correlation Matrix 
correlation_matrix = iris_df.corr() 
print("\nCorrelation Matrix:") 
print(correlation_matrix) 
 
# Pairplot 
import seaborn as sns 
sns.pairplot(iris_df, hue='target') 
plt.show()





EXPERIMENT 8
------------

=> AIM
Write a Python Program to implement Simple and Multiple Linear Regression.


=> THEORY
Simple Linear Regression is a fundamental statistical method for modeling the relationship between a dependent variable and a single independent variable. The primary goal is to understand and predict the behavior of the dependent variable based on changes in the independent variable. In essence, it seeks to fit a linear line that best captures the relationship between the two variables. The model is characterized by an intercept and slope, representing the starting point and the rate of change of the linear relationship. Multiple Linear Regression extends the principles of simple linear regression to incorporate multiple independent variables. It aims to model the relationship between a dependent variable and two or more predictors. The model includes an intercept and coefficients for each predictor, reflecting their individual contributions to the dependent variable. Multiple Linear Regression is valuable when dealing with more complex scenarios where multiple factors may influence the outcome simultaneously. Interpretation involves understanding how changes in each predictor, holding others constant, impact the dependent variable.


=> CODE
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression 
from sklearn.metrics import mean_squared_error 
# Load the Iris dataset 
iris = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', 
                   header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']) 
# Consider sepal length as the target variable 
X_simple = iris[['sepal_width']]  # Feature for simple linear regression 
X_multiple = iris[['sepal_width', 'petal_length', 'petal_width']]  # Features for multiple linear regression 
y = iris['sepal_length'] 
# Simple Linear Regression 
X_simple_train, X_simple_test, y_train, y_test = train_test_split(X_simple, y, test_size=0.2, 
random_state=42) 
simple_reg_model = LinearRegression() 
simple_reg_model.fit(X_simple_train, y_train) 
y_simple_pred = simple_reg_model.predict(X_simple_test) 
# Display results for Simple Linear Regression 
print("Simple Linear Regression:") 
print("Coefficients:", simple_reg_model.coef_) 
print("Intercept:", simple_reg_model.intercept_) 
print("Mean Squared Error:", mean_squared_error(y_test, y_simple_pred)) 
 
# Multiple Linear Regression 
X_multiple_train, X_multiple_test, y_train, y_test = train_test_split(X_multiple, y, test_size=0.2, 
random_state=42) 
multiple_reg_model = LinearRegression() 
multiple_reg_model.fit(X_multiple_train, y_train) 
y_multiple_pred = multiple_reg_model.predict(X_multiple_test) 
# Display results for Multiple Linear Regression 
print("\nMultiple Linear Regression:") 
print("Coefficients:", multiple_reg_model.coef_) 
print("Intercept:", multiple_reg_model.intercept_) 
print("Mean Squared Error:", mean_squared_error(y_test, y_multiple_pred)) 
 
# Visualize the predictions for Simple Linear Regression 
plt.scatter(X_simple_test, y_test, color='black', label='Actual') 
plt.plot(X_simple_test, y_simple_pred, color='blue', linewidth=3, label='Simple Linear Regression') 
plt.xlabel('Sepal Width') 
plt.ylabel('Sepal Length') 
plt.title('Simple Linear Regression on Iris Dataset') 
plt.legend() 
plt.show() 
# Visualize the predictions for Multiple Linear Regression 
fig = plt.figure(figsize=(15, 5)) 
ax1 = fig.add_subplot(131, projection='3d') 
ax1.scatter(X_multiple_test['sepal_width'], X_multiple_test['petal_length'], y_test, color='black', 
label='Actual') 
ax1.scatter(X_multiple_test['sepal_width'], X_multiple_test['petal_length'], y_multiple_pred, color='red', 
s=80, label='Prediction') 
ax1.set_xlabel('Sepal Width') 
ax1.set_ylabel('Petal Length') 
ax1.set_zlabel('Sepal Length') 
ax1.set_title('Multiple Linear Regression (3D Visualization)') 
ax1.legend() 
ax2 = fig.add_subplot(132) 
ax2.scatter(X_multiple_test['petal_length'], y_test, color='black', label='Actual') 
ax2.scatter(X_multiple_test['petal_length'], y_multiple_pred, color='red', s=80, label='Prediction') 
ax2.set_xlabel('Petal Length') 
ax2.set_ylabel('Sepal Length') 
ax2.set_title('Multiple Linear Regression (2D Visualization)') 
ax2.legend() 
ax3 = fig.add_subplot(133) 
ax3.scatter(X_multiple_test['petal_width'], y_test, color='black', label='Actual') 
ax3.scatter(X_multiple_test['petal_width'], y_multiple_pred, color='red', s=80, label='Prediction') 
ax3.set_xlabel('Petal Width') 
ax3.set_ylabel('Sepal Length') 
ax3.set_title('Multiple Linear Regression (2D Visualization)') 
ax3.legend() 
plt.tight_layout() 
plt.show()





EXPERIMENT 9
------------

=> AIM
Write a Python Program to implement Logistic Regression on a given dataset.


=> THEORY
Logistic Regression is a widely used statistical method for binary classification tasks. It models the relationship between a binary dependent variable (0 or 1) and one or more independent variables. Unlike linear regression, it employs the logistic function (sigmoid) to map a linear 
combination of predictors to a probability between 0 and 1.  
 
The key idea is to estimate the probability of the event occurring. Predictions are made by setting a threshold on the predicted probabilities, commonly 0.5. Logistic Regression is interpretable, provides insights into the impact of predictors on the outcome, and is valuable in scenarios such as medical diagnosis, credit scoring, and marketing analytics.


=> CODE
import numpy as np 
def sigmoid(z): 
  return 1 / (1 + np.exp(-z)) 
 
class LogisticRegression: 
  def __init__(self, learning_rate=0.01, n_iters=100): 
    self.learning_rate = learning_rate 
    self.n_iters = n_iters 
    self.weights = None 
 
  def fit(self, X, y): 
    X = np.hstack((np.ones(X.shape[0]).reshape(-1, 1), X)) 
    self.weights = np.random.randn(X.shape[1]) 
    for _ in range(self.n_iters): 
      y_pred = sigmoid(X.dot(self.weights)) 
      y_error = y - y_pred 
      self.weights += self.learning_rate * X.T.dot(y_error) 
 
  def predict(self, X): 
    X = np.hstack((np.ones(X.shape[0]).reshape(-1, 1), X)) 
    y_pred = sigmoid(X.dot(self.weights)) 
    y_pred = np.where(y_pred > 0.5, 1, 0) 
    return y_pred 
 
# Load the Iris dataset 
from sklearn.datasets import load_iris 
iris = load_iris() 
X, y = iris.data, iris.target 
 
# Split data into train and test sets 
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) 
 
# Train the model 
model = LogisticRegression() 
model.fit(X_train, y_train) 
 
# Predict on test data 
y_pred = model.predict(X_test) 
 
# Evaluate accuracy 
from sklearn.metrics import accuracy_score 
accuracy = accuracy_score(y_test, y_pred) 
print(f"Accuracy: {accuracy:.4f}")





EXPERIMENT 10
-------------

=> AIM
Write a Python Program to implement a decision tree  on a given data set.


=> THEORY
A Decision Tree is a versatile and interpretable machine learning algorithm used for both classification and regression tasks. The essence of a Decision Tree lies in its hierarchical structure, resembling a tree, where each internal node represents a decision based on a feature, and each leaf node corresponds to an outcome or a prediction. The algorithm recursively splits the dataset into subsets based on the most discriminative features, creating decision nodes. These nodes guide the traversal of the tree until a leaf node is reached, providing the final prediction or decision. 
The primary advantage of Decision Trees lies in their ability to handle both numerical and categorical data, as well as their interpretability, allowing users to easily understand the decision making process.


=> CODE
import numpy as np 
import pandas as pd 
 
class DecisionTree: 
    def __init__(self, max_depth=None): 
        self.max_depth = max_depth 
        self.tree = None 
    def entropy(self, y): 
        _, counts = np.unique(y, return_counts=True) 
        probabilities = counts / len(y) 
        return -np.sum(probabilities * np.log2(probabilities + 1e-10)) 
    def information_gain(self, X, y, feature, threshold): 
        mask = X[:, feature] <= threshold 
        left_entropy = self.entropy(y[mask]) 
        right_entropy = self.entropy(y[~mask]) 
 
        total_entropy = self.entropy(y) 
        weighted_entropy = (len(y[mask]) / len(y)) * left_entropy + (len(y[~mask]) / len(y)) * right_entropy 
        return total_entropy - weighted_entropy 
    def find_best_split(self, X, y): 
        num_features = X.shape[1] 
        best_feature, best_threshold, max_gain = None, None, -1 
        for feature in range(num_features): 
            unique_values = np.unique(X[:, feature]) 
            thresholds = (unique_values[:-1] + unique_values[1:]) / 2 
            for threshold in thresholds: 
                gain = self.information_gain(X, y, feature, threshold) 
                if gain > max_gain: 
                    best_feature, best_threshold, max_gain = feature, threshold, gain 
        return best_feature, best_threshold 
 
    def build_tree(self, X, y, depth=0): 
        num_samples, num_features = X.shape 
        unique_classes, class_counts = np.unique(y, return_counts=True) 
        # Base cases 
        if len(unique_classes) == 1: 
            return {'class': unique_classes[0]} 
        if depth == self.max_depth: 
            return {'class': unique_classes[np.argmax(class_counts)]} 
        # Recursive case 
        best_feature, best_threshold = self.find_best_split(X, y) 
        if best_feature is not None: 
            mask = X[:, best_feature] <= best_threshold 
            left_subtree = self.build_tree(X[mask], y[mask], depth + 1) 
            right_subtree = self.build_tree(X[~mask], y[~mask], depth + 1) 
            return {'feature': best_feature, 'threshold': best_threshold, 
                    'left': left_subtree, 'right': right_subtree} 
        else: 
            return {'class': unique_classes[np.argmax(class_counts)]} 
    def fit(self, X, y): 
        self.tree = self.build_tree(X, y) 
    def predict_instance(self, x, tree): 
        if 'class' in tree: 
            return tree['class'] 
        else: 
            if x[tree['feature']] <= tree['threshold']: 
                return self.predict_instance(x, tree['left']) 
            else: 
                return self.predict_instance(x, tree['right']) 
    def predict(self, X): 
        if self.tree is None: 
            raise ValueError("The model is not trained. Call fit() before predict().") 
 
        predictions = [self.predict_instance(x, self.tree) for x in X] 
        return np.array(predictions) 
 
# Example usage with Iris dataset 
from sklearn.datasets import load_iris 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import accuracy_score 
 
# Load the Iris dataset 
iris = load_iris() 
X, y = iris.data, iris.target 
 
# Split the dataset into training and testing sets 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 
# Instantiate and train the decision tree 
dt = DecisionTree(max_depth=3) 
dt.fit(X_train, y_train) 
 
y_pred = dt.predict(X_test) 
 
# Evaluate accuracy 
accuracy = accuracy_score(y_test, y_pred) 
print("Accuracy:", accuracy)





EXPERIMENT 11
-------------

=> AIM
Write a Python Program to implement  K-means clustering algorithm.


=> THEORY
K-Means Clustering- is a popular unsupervised machine learning algorithm designed for partitioning a dataset into distinct groups, or clusters. The algorithm operates iteratively to assign data points to clusters based on their proximity to the cluster's centroid. The "K" in K-Means refers to the predetermined number of clusters that the algorithm seeks to identify in the data.  
 
K-Means is particularly effective-when the number of clusters is known or can be estimated. However, the algorithm is sensitive to the initial placement of centroids and may converge to local optima.


=> CODE
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.cluster import KMeans 
from sklearn import datasets 
from sklearn.decomposition import PCA 
 
# Load the Iris dataset 
iris = datasets.load_iris() 
X = iris.data 
 
# Choose the number of clusters (k) 
k = 3 
 
# Apply KMeans clustering 
kmeans = KMeans(n_clusters=k, random_state=42) 
kmeans.fit(X) 
labels = kmeans.labels_ 
centroids = kmeans.cluster_centers_ 
 
# Reduce data dimensionality for visualization (using PCA) 
pca = PCA(n_components=2) 
X_pca = pca.fit_transform(X) 
centroids_pca = pca.transform(centroids) 
 
# Visualize the clusters 
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', label='Data Points') 
plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], s=300, marker='X', c='red', label='Centroids') 
plt.title('K-means Clustering on Iris Dataset') 
plt.xlabel('Principal Component 1') 
plt.ylabel('Principal Component 2') 
plt.legend() 
plt.show()





EXPERIMENT 12
--------------

=> AIM
Write a program to generate a Stem and leaf plot


=> THEORY
A Stem-and-Leaf Plot is a graphical representation used for visualizing the distribution of a dataset. This method preserves the raw data while organizing it clearly and concisely. In a stem and-leaf plot, each data point is separated into a "stem" and a "leaf."  
The stem consists of the leading digits, and the leaf represents the trailing digit(s). Stem-and-leaf plots are especially useful for small to moderately sized datasets, providing a visual summary of the data's distribution.


=> CODE
import matplotlib.pyplot as plt 
from sklearn import datasets 
 
# Load the Iris dataset 
iris = datasets.load_iris() 
iris_data = iris.data 
sepal_length = iris_data[:, 0]  # Sepal length is the first column 
 
# Function to create a stem-and-leaf plot 
def stem_and_leaf(data): 
    stems = set(int(x) for x in data // 1) 
    stems = sorted(list(stems)) 
     
    for stem in stems: 
        leaves = [int(leaf) for leaf in data if int(leaf // 1) == stem] 
        print(f"{stem} | {' '.join(map(str, sorted(leaves)))}") 
 
# Generate the stem-and-leaf plot for sepal length 
print("Stem-and-Leaf Plot for Sepal Length:") 
stem_and_leaf(sepal_length) 
 
# Plot the stem-and-leaf plot using matplotlib 
plt.stem(sepal_length) 
plt.xlabel('Index') 
plt.ylabel('Sepal Length') 
plt.title('Stem-and-Leaf Plot for Sepal Length') 
plt.show()





EXPERIMENT 13
-------------

=> AIM
Write a program for ANOVA test


=> THEORY
Analysis of Variance (ANOVA) -is a statistical technique used to compare means across multiple groups to determine if there are significant differences. ANOVA assesses the variance within and between groups to infer whether the observed differences in means are likely due to true variations or random chance. The method is applicable when dealing with categorical independent variables and continuous dependent variables. 
 Two components- variance within groups and variance between groups. The F-statistic, which is the ratio of between-group variance to within-group variance, is used to assess whether the means are significantly different. If the F-statistic is sufficiently large and the associated p-value is below a predefined significance level, it indicates that at least one group mean differs significantly from the others.


=> CODE
import numpy as np 
from scipy.stats import f_oneway 
from sklearn import datasets 
 
# Load the Iris dataset 
iris = datasets.load_iris() 
iris_data = iris.data 
target = iris.target 
 
# Extract data for each species (setosa, versicolor, and virginica) 
setosa_data = iris_data[target == 0] 
versicolor_data = iris_data[target == 1] 
virginica_data = iris_data[target == 2] 
 
# Perform one-way ANOVA 
f_statistic, p_value_anova = f_oneway(setosa_data, versicolor_data, virginica_data) 
 
# Print results 
print(f"ANOVA: F-statistic = {f_statistic}, p-values = {p_value_anova}") 
 
# Interpret the results 
alpha = 0.05 
if any(p_value < alpha for p_value in p_value_anova): 
    print("At least one pair of groups has significantly different means.") 
else: 
    print("There is no significant difference in the means of the groups.")





EXPERIMENT 14
-------------

=> AIM
Write a program for z-testing and t-testing


=> THEORY
Z-Testing and T-Testing - are statistical methods used for hypothesis testing to assess whether observed data provides sufficient evidence to reject a null hypothesis.  Z-Testing- is employed when the population standard deviation is known. The Z-test statistic is calculated by comparing the difference between the sample mean and the hypothesized population mean to the standard error.  The result is then compared to the standard normal distribution (Z-distribution) to determine statistical significance.  
 
T-Testing- on the other hand, is suitable when the population standard deviation is unknown, and it is estimated from the sample. The t-test statistic is calculated using the sample mean, hypothesized population mean, sample standard deviation, and sample size.  The result is compared to the t-distribution, which has heavier tails compared to the normal distribution, making it appropriate for smaller sample sizes.


=> CODE
import numpy as np 
from scipy import stats 
from sklearn import datasets 
 
# Load the Iris dataset 
iris = datasets.load_iris() 
iris_data = iris.data 
target = iris.target 
 
# Extract two groups for testing (assuming petal length for this example) 
group1 = iris_data[target == 0, 2]  # Petal length for setosa 
group2 = iris_data[target == 1, 2]  # Petal length for versicolor 
 
# Z-testing manually 
def z_test(group1, group2): 
    p1 = np.mean(group1) 
    p2 = np.mean(group2) 
    n1 = len(group1) 
    n2 = len(group2) 
    p_combined = (p1 * n1 + p2 * n2) / (n1 + n2) 
    se = np.sqrt(p_combined * (1 - p_combined) * (1 / n1 + 1 / n2)) 
    z_stat = (p1 - p2) / se 
    p_value_z = 2 * (1 - stats.norm.cdf(np.abs(z_stat))) 
    return z_stat, p_value_z 
 
# T-testing manually 
def t_test(group1, group2): 
    t_stat, p_value_t = stats.ttest_ind(group1, group2) 
    return t_stat, p_value_t 
 
# Perform tests 
z_stat, p_value_z = z_test(group1, group2) 
t_stat, p_value_t = t_test(group1, group2) 
 
# Print results 
print(f"Z-Test: Z-statistic = {z_stat}, p-value = {p_value_z}") 
print(f"T-Test: T-statistic = {t_stat}, p-value = {p_value_t}")
